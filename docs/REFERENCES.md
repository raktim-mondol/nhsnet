# NHS-Net References

## 1. Hebbian Learning

### Original Theory
- Hebb, D. O. (1949). The Organization of Behavior: A Neuropsychological Theory. Wiley.
  - Seminal text introducing Hebb's rule: "neurons that fire together, wire together"

### Modern Computational Adaptation
- Löwel, S., & Singer, W. (1992). Selection of Intrinsic Horizontal Connections in the Visual Cortex by Correlated Neuronal Activity. Science.
- Oja, E. (1982). A Simplified Neuron Model as a Principal Component Analyzer. Journal of Mathematical Biology.

### Hebbian Learning in Deep Networks
- Lillicrap, T. P., et al. (2016). Random Synaptic Feedback Weights Support Error Backpropagation for Deep Learning. Nature Communications.

## 2. Neurogenesis

### Biological Basis
- Eriksson, P. S., et al. (1998). Neurogenesis in the Adult Human Hippocampus. Nature Medicine.
- Kempermann, G., et al. (2004). Functional Significance of Adult Neurogenesis. Current Opinion in Neurobiology.

### Computational Inspiration
- Miconi, T., et al. (2018). Differentiable Plasticity: Training Plastic Neural Networks with Backpropagation. ICML.
- Li, Y., et al. (2019). Dynamic Network Expansion for Continual Learning. NeurIPS.

## 3. Synaptic Pruning

### Biological Basis
- Huttenlocher, P. R. (1979). Synaptic Density in Human Frontal Cortex – Developmental Changes and Effects of Aging. Brain Research.

### Pruning in Neural Networks
- Han, S., et al. (2015). Deep Compression: Compressing Deep Neural Networks with Pruning, Trained Quantization, and Huffman Coding. ICLR.
- Frankle, J., & Carbin, M. (2019). The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks. ICLR.
- Gale, T., et al. (2019). The State of Sparsity in Deep Neural Networks. ICML.

## 4. Sparse Neural Circuitry

### Biological Observations
- Ramon y Cajal, S. (1911). Histologie du Système Nerveux de l'Homme et des Vertébrés. Maloine.
- Olshausen, B. A., & Field, D. J. (1996). Emergence of Simple-Cell Receptive Field Properties by Learning a Sparse Code for Natural Images. Nature.

### Sparse Deep Learning
- Liu, Z., et al. (2021). SparseConv: Efficient Sparse Convolutional Neural Networks. CVPR.
- Makhzani, A., & Frey, B. (2015). k-Sparse Autoencoders. ICLR.

## 5. Hodgkin-Huxley Model

### Original Theory
- Hodgkin, A. L., & Huxley, A. F. (1952). A Quantitative Description of Membrane Current and Its Application to Conduction and Excitation in Nerve. Journal of Physiology.

### Simplified Computational Adaptations
- Yang, G. R., et al. (2020). Task Representations in Neural Networks Trained to Perform Many Cognitive Tasks. Nature Neuroscience.
- Bellec, G., et al. (2020). A Solution to the Learning Dilemma for Recurrent Networks of Spiking Neurons. Nature Communications.

## 6. Dynamic Architecture Growth
- Chen, Y., et al. (2016). Net2Net: Accelerating Learning via Knowledge Transfer. ICLR.

## 7. General References

### Textbooks
- Kandel, E. R., et al. (2021). Principles of Neural Science, 6th Edition. McGraw-Hill.
- Goodfellow, I., et al. (2016). Deep Learning. MIT Press (Chapter on Biologically Inspired Models).